1st EXP

  import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
np.random.seed(42)
data = {'X': np.random.rand(100, 1).flatten()}
data['y']= 3 * data['X'] + 2 + 0.1 * np.random.randn(100)
df = pd.DataFrame(data)
print(df.info())

X_train, X_test, y_train, y_test = train_test_split(df['X'], df['y'], test_size=0.2, random_state=42)
model = Sequential()
# Add a hidden layer with 5 units and ReLU activation
model.add(Dense(units=6, activation='relu')) 
# Add the output layer with 1 unit and linear activation
model.add(Dense(units=1, activation='linear'))
# Build the model
model.build(input_shape=(None, 1))
# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error')
# Display the model summary
model.summary()
#@title Train, Predict, and Evaluate Neural Network
# Train the model
history = model.fit(X_train, y_train, epochs=100, verbose=0)
# Make predictions on the test set
y_pred = model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set: {mse:.4f}')
#@title Visualize Results of Simple Linear Regression with Neural Network
plt.scatter(X_test, y_test, color='black', label='Actual')
plt.plot(X_test, y_pred, color='blue',  label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Scatter Plot of Actual vs Predicted Values (Simple Linear Regression)')
plt.legend()
plt.show()







  4th EXP

  import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
sns.set()
tf.__version__
data = pd.read_csv(r"C:\Users\91939\Desktop\3-2\DL\heart dataset.csv")
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']
X = pd.get_dummies(X, columns=['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model = Sequential([Dense(300, activation='relu', input_shape=(X_train.shape[1],)),
                    Dense(150, activation='relu'),
                    Dense(75, activation='relu'),
                    Dense(1, activation='sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
y_train_binary = y_train.astype('float32')
history = model.fit(X_train_scaled, y_train_binary, epochs=10,verbose=2)
model.summary()
print("List of layers:", model.layers)
print("\nName of the second layer:", model.layers[1].name)
hidden2 = model.layers[2]
weights, bias = hidden2.get_weights()
print("Weights:", weights)
print("Bias:", bias)
pd.DataFrame(history.history).plot()
plt.xlabel("Epoch")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
plt.show()
loss, acc = model.evaluate(X_test_scaled, y_test)
print("Test Test Loss:",loss)
print("Test Accuracy:", acc)


7th EXP

import keras,os
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from keras.preprocessing.image import ImageDataGenerator
import numpy as np
TRAIN_DIR = r"C:\Users\Keerthi Mupparaju\Desktop\DL\train"
TEST_DIR = r"C:\Users\Keerthi Mupparaju\Desktop\DL\test"
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 2
IMAGE_CHANNELS=3
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.1  # 10% of the data will be used for validation

)

test_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset="training", 
    shuffle=True # Subset for training data
)

validation_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset="validation",  # Subset for validation data
)

test_generator = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary'
)
model = Sequential()
model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Flatten())
model.add(Dense(units=4096,activation="relu"))
model.add(Dense(units=4096,activation="relu"))
model.add(Dense(units=1, activation="sigmoid"))
model.compile(optimizer='adam', loss=keras.losses.binary_crossentropy, metrics=['accuracy'])
model.summary()
import pandas as pd
history_df = pd.DataFrame(hist.history)
history_df
history_df.loc[:,['loss','accuracy']].plot()
history_df.loc[:,['val_loss','val_accuracy']].plot()

8 EXP

  import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt  
# Load dataset
spotify = pd.read_csv(r"C:\Users\91939\Desktop\3-2\DL\spotify.csv")
# Prepare features and target
X = spotify.dropna().copy()
y = X.pop('track_popularity') / 100
artists = X['track_artist']
# Define features
features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',
                'speechiness', 'acousticness', 'instrumentalness',
                'liveness', 'valence', 'tempo', 'duration_ms']
features_cat = ['playlist_genre']
# Preprocess features
preprocessor = make_column_transformer(
    (StandardScaler(), features_num),
    (OneHotEncoder(), features_cat),
 )
X = preprocessor.fit_transform(X)
# Split dataset
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
# Define model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=[X_train.shape[1]]),
    # layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    # layers.Dropout(0.3),
    layers.Dense(1)
 ])
model.compile(optimizer='adam', loss='mae')
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
    verbose=1
 )
 # Evaluate model on test data
test_loss = model.evaluate(X_test, y_test)
print("Test Loss (MAE): {:.4f}".format(test_loss))
# Additional evaluation metrics
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE): {:.4f}".format(mae))
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
#print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()))
# Define model
model_D_and_E = keras.Sequential([
 layers.Dense(128, activation='relu', input_shape=[X_train.shape[1]]),
 layers.Dropout(0.3),
 layers.Dense(64, activation='relu'),
 layers.Dropout(0.3),
 layers.Dense(1)
 ])
# Compile model
model_D_and_E.compile(optimizer='adam', loss='mae')
# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True)
# Train model
history = model_D_and_E.fit(
 X_train, y_train,
 validation_data=(X_valid, y_valid),
 batch_size=512,
 epochs=50,
 callbacks=[early_stopping],
 verbose=1
 )
# Evaluate model on test data
test_loss = model_D_and_E.evaluate(X_test, y_test)
print("Test Loss (MAE): {:.4f}".format(test_loss))
# Additional evaluation metrics
y_pred = model_D_and_E.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE): {:.4f}".format(mae))
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
#print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()))
plt.plot(history_df['val_loss'])
plt.plot(history_df['loss'])
plt.show()

9th EXP

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
# Load the weather dataset
weather_data = pd.read_csv(r"C:\Users\91939\Desktop\3-2\DL\weather_features.csv")
# Select relevant columns for analysis
selected_features = ['temp', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'rain_3h', 'snow_3h', 'clouds_all']
X = weather_data[selected_features].values
# Normalize the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
# Define the input sequence length
seq_length = 10
# Prepare the data for training
X_train = []
y_train = []
for i in range(len(X_scaled) - seq_length):
    X_train.append(X_scaled[i:i + seq_length])
    y_train.append(X_scaled[i + seq_length])
X_train, y_train = np.array(X_train), np.array(y_train)
# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Define the RNN model
model = Sequential([
SimpleRNN(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
Dropout(0.2),
SimpleRNN(32),
Dropout(0.2),
Dense(1)
 ])
# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')
# Define early stopping
early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True)
# Train the model with early stopping
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stop])
# Evaluate the model on both training and validation data
train_loss = model.evaluate(X_train, y_train)
val_loss = model.evaluate(X_val, y_val)
print("Training Loss:", train_loss)
print("Validation Loss:", val_loss)
# Visualize the training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()


3rd exp

import numpy as np
import pandas as pd
# import requests
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression , Ridge , Lasso
# from io import StringIO


boston_pd = pd.read_csv("housing.csv", delim_whitespace=True, header=None)
# boston_pd.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'House Price']

X = boston_pd.iloc[:, :-1]
Y = boston_pd.iloc[:, -1]
print(boston_pd)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)
lreg = LinearRegression()
lreg.fit(x_train, y_train)
lreg_y_pred = lreg.predict(x_test)

mean_squared_error = np.mean((lreg_y_pred - y_test) ** 2)
print("Mean squared Error on test set:", mean_squared_error)

lreg_coefficient = pd.DataFrame()
lreg_coefficient["Columns"] = x_train.columns
lreg_coefficient["Coefficient Estimate"] = pd.Series(lreg.coef_)
print(lreg_coefficient)

fig, ax = plt.subplots(figsize=(20, 10))
ax.bar(lreg_coefficient["Columns"], lreg_coefficient['Coefficient Estimate'])
plt.show()

ridgeR=Ridge(alpha = 1)
ridgeR.fit(x_train, y_train)
y_pred=ridgeR.predict(x_test)

mean_squared_error_ridge=np.mean((y_pred-y_test)**2)
print("Mean squared error on test set",mean_squared_error_ridge)
ridge_coefficient=pd.DataFrame()
ridge_coefficient["Columns"]= x_train.columns
ridge_coefficient['Coefficient Estimate']=pd.Series(ridgeR.coef_)
print(ridge_coefficient)
fig, ax = plt.subplots(figsize=(20, 10))
ax.bar(lreg_coefficient["Columns"], lreg_coefficient['Coefficient Estimate'])
plt.show()

lasso = Lasso(alpha = 1)
lasso.fit(x_train, y_train)
y_pred1 = lasso.predict(x_test)
mean_squared_error = np.mean((y_pred1 - y_test)**2)
print("Mean squared error on test set", mean_squared_error)
lasso_coeff=pd.DataFrame()
lasso_coeff["Columns"]=x_train.columns
lasso_coeff["Coefficient Estimate"] = pd.Series(lasso.coef_)
print (lasso_coeff)
